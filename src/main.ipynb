{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Import dependencies.\n",
    "\n",
    "Moduels:\n",
    "    tensorflow: The neural network framework.\n",
    "    layer: The customized single layers.\n",
    "    module: The customized multi-layer modules.\n",
    "    \n",
    "    os: The basic module for path parsing.\n",
    "    json: The basic modue for config parsing.\n",
    "    zipfile: Extract zip file.\n",
    "    random: Shuffle indices.\n",
    "    scipy.io: Load .mat file.\n",
    "    numpy: To process .mat file data.\n",
    "    \n",
    "    tqdm: The 3rd-party looping visualzer.\n",
    "'''\n",
    "\n",
    "import layer, module\n",
    "import tensorflow as tf\n",
    "import os, json, zipfile, random\n",
    "import scipy.io, scipy.misc, imageio, math, numpy as np\n",
    "from enum import Enum\n",
    "from tqdm import tqdm, trange\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Parsing config.\n",
    "\n",
    "The config file is saved to root/config.\n",
    "'''\n",
    "CONFIG_PATH = os.path.abspath('../config.json')\n",
    "with open(CONFIG_PATH) as CONFIG_FILE:\n",
    "    CONFIG = json.loads(CONFIG_FILE.read())\n",
    "    flags = tf.app.flags\n",
    "    flags.DEFINE_string('project', CONFIG['project'], 'The project name.')\n",
    "    \n",
    "    flags.DEFINE_string('path_to_pretrained',\n",
    "                        os.path.join(\n",
    "                            os.path.expanduser(CONFIG['pretrained']['path']),\n",
    "                            CONFIG['project']),\n",
    "                        'The path to pretrained parameters.')\n",
    "    flags.DEFINE_boolean('load_pretrained', CONFIG['pretrained']['load'], 'Load the latest pretrained parameters.')\n",
    "    flags.DEFINE_boolean('save_pretrained', CONFIG['pretrained']['save'], 'Save the trained parameters.')\n",
    "    \n",
    "    flags.DEFINE_string('path_to_log',\n",
    "                        os.path.join(\n",
    "                            os.path.expanduser(CONFIG['log']['path']),\n",
    "                            CONFIG['project']),\n",
    "                        'The path to log.')\n",
    "    flags.DEFINE_boolean('save_log', CONFIG['log']['save'], 'Save the log.')\n",
    "    \n",
    "    flags.DEFINE_string('path_to_data',\n",
    "                        os.path.expanduser(CONFIG['data']['path']),\n",
    "                        'The path to data.')\n",
    "    flags.DEFINE_string('name_of_data', CONFIG['data']['name'], 'The name of data.')\n",
    "    \n",
    "    flags.DEFINE_string('task', CONFIG['task']['step'], 'The task to be done.')\n",
    "    flags.DEFINE_integer('epoch', CONFIG['task']['train']['epoch'], 'The epoch to be trained.')\n",
    "    flags.DEFINE_string('metric', CONFIG['task']['eval']['metric'], 'The evaluation metric.')\n",
    "    flags.DEFINE_float('metric_coefficient', CONFIG['task']['eval']['coefficient'], 'The evaluation metric coefficient.')\n",
    "    \n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "matlab_path = '/media/nulledge/UBUNTU 16_0/MPII/mpii_human_pose_v1_u12_2/mpii_human_pose_v1_u12_1.mat'\n",
    "annotation = scipy.io.loadmat(matlab_path)['RELEASE']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "img_train = annotation['img_train'][0, 0][0, :]\n",
    "valid_index = []\n",
    "\n",
    "img_idx = 24730\n",
    "r_idx = 1\n",
    "\n",
    "ref = annotation['annolist'][0, 0][0, img_idx]['annorect'][0, r_idx]['annopoints'][0, 0]['point'][0, :]\n",
    "\n",
    "for point in ref:\n",
    "    print(point.dtype)\n",
    "'''\n",
    "'''\n",
    "humans_in_image = self.__annotation['annolist'][0, 0][0, img_idx]['annorect'].shape[1]\n",
    "\n",
    "for r_idx in range(humans_in_image):\n",
    "    valid_index.append((img_idx, r_idx))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Common metadata.\n",
    "'''\n",
    "\n",
    "class DataInterface(object):\n",
    "    def __init__(self):\n",
    "        if self.__corrupted():\n",
    "            self.__reload()\n",
    "    \n",
    "    def __corrupted(self):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def __reload(self):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def getBatch(self, batch_size):\n",
    "        return [(lambda x: \n",
    "                 (self.__getImage(x),\n",
    "                 self.__getHeatmaps(x),\n",
    "                 self.__getExtra(x))\n",
    "                )(self.__getNext()) in range(batch_size)]\n",
    "        \n",
    "    def __getImage(self, index):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def __getHeatmaps(self, index):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def __getExtra(self, index):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def __getNext(self):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "class DataCenter(object):\n",
    "    def __init__(self, root):\n",
    "        self.__root = root\n",
    "    \n",
    "    def request(self, data, task):\n",
    "        path = os.path.join(self.__root, data)\n",
    "        \n",
    "        if data == 'FLIC':\n",
    "            return FLIC(root = path, task = task)\n",
    "        elif data == 'MPII':\n",
    "            return MPII(root = path, task = task)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "class Joint(Enum):\n",
    "\n",
    "    # FLIC\n",
    "    L_Shoulder  =   1\n",
    "    R_Shoulder  =   2\n",
    "    L_Elbow     =   3\n",
    "    R_Elbow     =   4\n",
    "    L_Wrist     =   5\n",
    "    R_Wrist     =   6\n",
    "    L_Hip       =   7\n",
    "    R_Hip       =   8\n",
    "    L_Knee      =   9\n",
    "    R_Knee      =   10\n",
    "    L_Ankle     =   11\n",
    "    R_Ankle     =   12\n",
    "\n",
    "    L_Eye       =   13\n",
    "    R_Eye       =   14\n",
    "    L_Ear       =   15\n",
    "    R_Ear       =   16\n",
    "    M_Nose      =   17\n",
    "\n",
    "    M_Shoulder  =   18\n",
    "    M_Hip       =   19\n",
    "    M_Ear       =   20\n",
    "    M_Torso     =   21\n",
    "    M_LUpperArm =   22\n",
    "    M_RUpperArm =   23\n",
    "    M_LLowerArm =   24\n",
    "    M_RLowerArm =   25\n",
    "    M_LUpperLeg =   26\n",
    "    M_RUpperLeg =   27\n",
    "    M_LLowerLeg =   28\n",
    "    M_RLowerLeg =   29\n",
    "\n",
    "    # MPII\n",
    "    # M_Pelvis    =   M_Hip\n",
    "    # M_Thorax    =   M_Torso\n",
    "    M_UpperNeck =   30\n",
    "    M_HeadTop   =   31\n",
    "    \n",
    "class Basis(Enum):\n",
    "    x = 0\n",
    "    y = 1\n",
    "    \n",
    "def squeeze(annotation, indices):\n",
    "    if len(indices) == 0:\n",
    "        return np.squeeze(annotation)\n",
    "    return squeeze(np.squeeze(annotation)[indices[0]], indices[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' FLIC data reader\n",
    "'''       \n",
    "class FLIC(DataInterface):\n",
    "    \n",
    "    NUMBER_OF_DATA = 5003\n",
    "    TRAIN_RATIO = 0.9\n",
    "    EVAL_RATIO = 1.0 - TRAIN_RATIO\n",
    "    \n",
    "    joint2index = {\n",
    "        Joint.L_Shoulder: 0,\n",
    "        Joint.L_Elbow: 1,\n",
    "        Joint.L_Wrist: 2,\n",
    "        Joint.R_Shoulder: 3,\n",
    "        Joint.R_Elbow: 4,\n",
    "        Joint.R_Wrist: 5,\n",
    "        Joint.L_Hip: 6,\n",
    "        Joint.R_Hip: 9,\n",
    "        Joint.L_Eye: 12,\n",
    "        Joint.R_Eye: 13,\n",
    "        Joint.M_Nose: 16\n",
    "    }\n",
    "    \n",
    "    def __init__(self, root, task):\n",
    "        self.__root = root\n",
    "        \n",
    "        if self.__corrupted():\n",
    "            self.__reload()\n",
    "            \n",
    "        # self.__extract_path = os.path.join(self.__root, 'FLIC')\n",
    "        self.__matlab_path = os.path.join(self.__extract_path, 'examples.mat')\n",
    "        self.__annotation = scipy.io.loadmat(self.__matlab_path)['examples']\n",
    "        self.__task = task\n",
    "        self.__index = { 'train': open(os.path.join(self.__root, 'train.txt'), 'r'),\n",
    "                       'eval': open(os.path.join(self.__root, 'eval.txt'), 'r')}\n",
    "        \n",
    "    def __delete__(self):\n",
    "        self.__index['train'].close()\n",
    "        self.__index['eval'].close()\n",
    "    \n",
    "    def __corrupted(self):\n",
    "        self.__archive_path = os.path.join(self.__root, 'FLIC.zip')\n",
    "        print('Check if the archive file exists...', end = '')\n",
    "        if not os.path.exists(self.__archive_path):\n",
    "            raise Exception('You have to download the archive file from https://bensapp.github.io/flic-dataset.html')\n",
    "        print('success!')\n",
    "        print('\\tpath:', self.__archive_path)\n",
    "        \n",
    "        \n",
    "        self.__extract_path = os.path.join(self.__root, 'FLIC')\n",
    "        print('Check if the archive file is extracted...', end = '')\n",
    "        if not os.path.exists(self.__extract_path):\n",
    "            print('failed!')\n",
    "            return True\n",
    "        print('success!')\n",
    "        print('\\tpath:', self.__extract_path)\n",
    "        \n",
    "        \n",
    "        self.__index_path = { 'train': os.path.join(self.__root, 'train.txt'),\n",
    "                             'eval': os.path.join(self.__root, 'eval.txt')}\n",
    "        print('Check if the index file is exists...', end = '')\n",
    "        if not (os.path.exists(self.__index_path['train']) and os.path.exists(self.__index_path['eval'])):\n",
    "            print('failed!')\n",
    "            return True\n",
    "        print('success!')\n",
    "        print('\\ttrain path:', self.__index_path['train'])\n",
    "        print('\\teval path:', self.__index_path['eval'])\n",
    "        \n",
    "        return False\n",
    "        \n",
    "    def __reload(self):\n",
    "        # archive_path = os.path.join(self.__data_path, FLIC.archive_file)\n",
    "        print('Extract the archive file...', end = '')\n",
    "        with zipfile.ZipFile(self.__archive_path, 'r') as archive_ref:\n",
    "            archive_ref.extractall(self.__root)\n",
    "        print('success!')\n",
    "        print('\\tpath:', self.__extract_path)\n",
    "        \n",
    "        \n",
    "        index_list = [i for i in range(FLIC.NUMBER_OF_DATA)]\n",
    "        random.shuffle(index_list)\n",
    "        print('Generate the random train/eval set...', end = '')\n",
    "        with open(self.__index_path['train'], 'w') as train_index:\n",
    "            for i in range(int(FLIC.TRAIN_RATIO * FLIC.NUMBER_OF_DATA)):\n",
    "                train_index.write(str(index_list[i]) + '\\n')\n",
    "                \n",
    "        with open(self.__index_path['eval'], 'w') as eval_index:\n",
    "            for i in range(int(FLIC.TRAIN_RATIO * FLIC.NUMBER_OF_DATA), FLIC.NUMBER_OF_DATA):\n",
    "                eval_index.write(str(index_list[i]) + '\\n')\n",
    "        print('success!')\n",
    "        print('\\ttrain path:', self.__index_path['train'])\n",
    "        print('\\teval path:', self.__index_path['eval'])\n",
    "        \n",
    "    def getBatch(self, batch_size):\n",
    "        \n",
    "        batch_index = []\n",
    "        \n",
    "        for _ in range(batch_size):\n",
    "            index = self.__getNext()\n",
    "            if index is None:\n",
    "                break\n",
    "            batch_index.append(index)\n",
    "        \n",
    "        batch_image = np.ndarray(shape = (len(batch_index), 256, 256, 3), dtype = np.float32)\n",
    "        for index in range(len(batch_index)):\n",
    "            batch_image[index][:, :, :] = self.__getImage(batch_index[index])\n",
    "            \n",
    "        batch_heat = np.ndarray(shape = (len(batch_index), 64, 64, len(Joint)), dtype = np.float32)\n",
    "        for index in range(len(batch_index)):\n",
    "            batch_heat[index][:, :, :] = self.__getHeat(batch_index[index])\n",
    "            \n",
    "        batch_extra = np.ndarray(shape = (len(batch_index), 1), dtype = np.float32)\n",
    "        for index in range(len(batch_index)):\n",
    "            batch_extra[index][:] = self.__getExtra(batch_index[index])\n",
    "            \n",
    "        return batch_image, batch_heat, batch_extra, FLIC.__getMasking()\n",
    "        \n",
    "            \n",
    "    def __getNext(self):\n",
    "        index = self.__index[self.__task].readline()\n",
    "        if index == '':\n",
    "            return None\n",
    "        return int(index)\n",
    "            \n",
    "    def __getImage(self, index):\n",
    "        torso = squeeze(self.__annotation, ['torsobox', index])\n",
    "        resolution = squeeze(self.__annotation, ['imgdims', index])\n",
    "        center = int((torso[2] + torso[0])/2)\n",
    "        \n",
    "        pad_left = 0\n",
    "        pad_right = 0 \n",
    "        if center + int(resolution[0]/2) > resolution[1]:\n",
    "            pad_right = center + int(resolution[0]/2) - resolution[1]\n",
    "        if center - int(resolution[0]/2) < 0:\n",
    "            pad_left = int(resolution[0]/2) - center\n",
    "            center += pad_left\n",
    "        \n",
    "        return scipy.misc.imresize(np.pad(\n",
    "            imageio.imread(\n",
    "                os.path.join(\n",
    "                    self.__extract_path,\n",
    "                    'images',\n",
    "                    squeeze(self.__annotation, ['filepath', index]).item()\n",
    "                )\n",
    "            ),\n",
    "            ((0, 0), (pad_left, pad_right), (0, 0)),\n",
    "            'constant', constant_values = (0, 0)\n",
    "        )[:, center - int(resolution[0]/2):center + int(resolution[0]/2),:], (256, 256))\n",
    "    \n",
    "    def __getHeat(self, index):\n",
    "        heatmaps = np.ndarray(shape = (64, 64, len(Joint)))\n",
    "        \n",
    "        \n",
    "        torso = squeeze(self.__annotation, ['torsobox', index])\n",
    "        resolution = squeeze(self.__annotation, ['imgdims', index])\n",
    "        center = int((torso[2] + torso[0])/2)\n",
    "        \n",
    "        pad_left = 0\n",
    "        if center - int(resolution[0]/2) < 0:\n",
    "            pad_left = int(resolution[0]/2) - center\n",
    "            center += pad_left\n",
    "        \n",
    "        \n",
    "        keypoints_ref = squeeze(self.__annotation, [index, 'coords'])\n",
    "        \n",
    "        for joint in Joint:\n",
    "            if joint not in FLIC.joint2index:\n",
    "                heatmaps[:, :, joint.value - 1] = 0\n",
    "            else:\n",
    "                scale = 64 / resolution[0]\n",
    "                heatmaps[:, :, joint.value - 1] = 255 * FLIC.__makeGaussian(64, 1,\n",
    "                      [(keypoints_ref[0][FLIC.joint2index[joint]] + pad_left - center + int(resolution[0]/2)) * scale,\n",
    "                      keypoints_ref[1][FLIC.joint2index[joint]] * scale])\n",
    "        return heatmaps\n",
    "    \n",
    "    def __makeGaussian(size, sigma = 1, center = None):\n",
    "\n",
    "        x = np.arange(0, size, 1, np.float32)\n",
    "        y = x[:,np.newaxis]\n",
    "\n",
    "        if center is None:\n",
    "            x0 = y0 = size // 2\n",
    "        else:\n",
    "            x0 = center[0]\n",
    "            y0 = center[1]\n",
    "\n",
    "        return 1/(sigma*np.sqrt(2*np.pi))*np.exp(-((x-x0)**2 + (y-y0)**2) / 2*sigma**2)\n",
    "    \n",
    "    def __getExtra(self, index):\n",
    "        torso = squeeze(self.__annotation, ['torsobox', index])\n",
    "        resolution = squeeze(self.__annotation, ['imgdims', index])\n",
    "        return np.linalg.norm(torso[2:4] - torso[0:2]) * 64 / resolution[0]\n",
    "    \n",
    "    def __getMasking():\n",
    "        return [(lambda joint: joint in FLIC.joint2index)(joint) for joint in Joint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' MPII data reader\n",
    "'''       \n",
    "class MPII(DataInterface):\n",
    "    \n",
    "    NUMBER_OF_DATA = -1\n",
    "    TRAIN_RATIO = 0.9\n",
    "    EVAL_RATIO = 1.0 - TRAIN_RATIO\n",
    "    \n",
    "    joint2index = {\n",
    "        Joint.R_Ankle: 0,\n",
    "        Joint.R_Knee: 1,\n",
    "        Joint.R_Hip: 2,\n",
    "        Joint.L_Hip: 3,\n",
    "        Joint.L_Knee: 4,\n",
    "        Joint.L_Ankle: 5,\n",
    "        # Joint.M_Pelvis: 6,\n",
    "        # Joint.M_Thorax: 7,\n",
    "        # Joint.M_UpperNeck: 8,\n",
    "        # Joint.M_HeadTop: 9,\n",
    "        Joint.R_Wrist: 10,\n",
    "        Joint.R_Elbow: 11,\n",
    "        Joint.R_Shoulder: 12,\n",
    "        Joint.L_Shoulder: 13,\n",
    "        Joint.L_Elbow: 14,\n",
    "        Joint.L_Wrist: 15\n",
    "    }\n",
    "    \n",
    "    def __init__(self, root, task):\n",
    "        self.__root = root\n",
    "        \n",
    "        if self.__corruptedExtraction():\n",
    "            raise Exception('You have to extrace the archive files')\n",
    "        \n",
    "        print('Load the annotation file...', end = '')\n",
    "        self.__matlab_path = os.path.join(self.__extract_path['annotation'], 'mpii_human_pose_v1_u12_1.mat')\n",
    "        self.__annotation = scipy.io.loadmat(self.__matlab_path)['RELEASE']\n",
    "        print('success!')\n",
    "        \n",
    "        if self.__corruptedIndex():\n",
    "            self.__reloadIndex()\n",
    "        \n",
    "        self.__task = task\n",
    "        self.__index = { 'train': open(os.path.join(self.__root, 'train.txt'), 'r'),\n",
    "                       'eval': open(os.path.join(self.__root, 'eval.txt'), 'r')}\n",
    "        \n",
    "        \n",
    "    def __delete__(self):\n",
    "        self.__index['train'].close()\n",
    "        self.__index['eval'].close()\n",
    "        \n",
    "        \n",
    "    def __corruptedExtraction(self):\n",
    "        self.__extract_path = { 'image': os.path.join(self.__root, 'images'),\n",
    "                               'annotation': os.path.join(self.__root, 'mpii_human_pose_v1_u12_2') }\n",
    "        print('Check if the archive file is extracted...', end = '')\n",
    "        if not (os.path.exists(self.__extract_path['image']) and os.path.exists(self.__extract_path['annotation'])):\n",
    "            raise Exception('You have to extract the archive files')\n",
    "        print('success!')\n",
    "        print('\\timage path:', self.__extract_path['image'])\n",
    "        print('\\tannotation path:', self.__extract_path['annotation'])\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    \n",
    "    def __corruptedIndex(self):\n",
    "        self.__index_path = { 'train': os.path.join(self.__root, 'train.txt'),\n",
    "                             'eval': os.path.join(self.__root, 'eval.txt')}\n",
    "        print('Check if the index file is exists...', end = '')\n",
    "        if not (os.path.exists(self.__index_path['train']) and os.path.exists(self.__index_path['eval'])):\n",
    "            print('failed!')\n",
    "            return True\n",
    "        print('success!')\n",
    "        print('\\ttrain path:', self.__index_path['train'])\n",
    "        print('\\teval path:', self.__index_path['eval'])\n",
    "        \n",
    "        return False\n",
    "    \n",
    "        \n",
    "    def __reloadIndex(self):\n",
    "        \n",
    "        img_train = self.__annotation['img_train'][0, 0][0, :]\n",
    "        valid_index = []\n",
    "\n",
    "        for img_idx in range(len(img_train)):\n",
    "            if img_train[img_idx] == 0:\n",
    "                continue\n",
    "            assert img_train[img_idx] == 1\n",
    "\n",
    "            # humans_in_image = self.__annotation['annolist'][0, 0][0, img_idx]['annorect'].shape[1]\n",
    "            humans_in_image = annotation['single_person'][0, 0][img_idx, 0]\n",
    "\n",
    "            if humans_in_image.shape[1] == 0:\n",
    "                continue\n",
    "\n",
    "            for r_idx in humans_in_image:\n",
    "                valid_index.append((img_idx, r_idx[0] - 1))\n",
    "\n",
    "        MPII.NUMBER_OF_DATA = len(valid_index)\n",
    "        \n",
    "        random.shuffle(valid_index)\n",
    "        \n",
    "        print('Generate the random train/eval set...', end = '')\n",
    "        with open(self.__index_path['train'], 'w') as train_index:\n",
    "            for i in range(int(MPII.TRAIN_RATIO * MPII.NUMBER_OF_DATA)):\n",
    "                train_index.write(str(valid_index[i][0]) + \" \" + str(valid_index[i][1]) + '\\n')\n",
    "                \n",
    "        with open(self.__index_path['eval'], 'w') as eval_index:\n",
    "            for i in range(int(MPII.TRAIN_RATIO * MPII.NUMBER_OF_DATA), MPII.NUMBER_OF_DATA):\n",
    "                eval_index.write(str(valid_index[i][0]) + \" \" + str(valid_index[i][1]) + '\\n')\n",
    "        print('success!')\n",
    "        print('\\ttrain path:', self.__index_path['train'])\n",
    "        print('\\teval path:', self.__index_path['eval'])\n",
    "        \n",
    "    def getBatch(self, batch_size):\n",
    "        \n",
    "        batch_index = []\n",
    "        \n",
    "        for _ in range(batch_size):\n",
    "            img_idx, r_idx = self.__getNext()\n",
    "            if img_idx is None or r_idx is None:\n",
    "                break\n",
    "            batch_index.append((img_idx, r_idx))\n",
    "        \n",
    "        batch_image = np.ndarray(shape = (len(batch_index), 256, 256, 3), dtype = np.float32)\n",
    "        for index in range(len(batch_index)):\n",
    "            batch_image[index][:, :, :] = self.__getImage(batch_index[index])\n",
    "            \n",
    "        batch_heat = np.ndarray(shape = (len(batch_index), 64, 64, len(Joint)), dtype = np.float32)\n",
    "        for index in range(len(batch_index)):\n",
    "            batch_heat[index][:, :, :] = self.__getHeat(batch_index[index])\n",
    "                 \n",
    "        batch_extra = np.ndarray(shape = (len(batch_index), 1), dtype = np.float32)\n",
    "        for index in range(len(batch_index)):\n",
    "            batch_extra[index][:] = self.__getExtra(batch_index[index])\n",
    "            \n",
    "        return batch_image, batch_heat, batch_extra, MPII.__getMasking()\n",
    "    \n",
    "    def resetBatch(self):\n",
    "        self.__index[self.__task].seek(0)\n",
    "        \n",
    "            \n",
    "    def __getNext(self):\n",
    "        index = self.__index[self.__task].readline()\n",
    "        if index == '':\n",
    "            return None, None\n",
    "        img_idx, r_idx = index.split(' ')\n",
    "        return int(img_idx), int(r_idx)\n",
    "    \n",
    "    \n",
    "    def resetBatch(self):\n",
    "        self.__index[self.__task].seek(0)\n",
    "    \n",
    "            \n",
    "    def __getImage(self, index):\n",
    "        img_idx, r_idx = index\n",
    "        \n",
    "        image_name = self.__annotation['annolist'][0, 0][0, img_idx]['image'][0, 0]['name'][0]\n",
    "        image_path = os.path.join(self.__extract_path['image'], image_name)\n",
    "        image_res = (int(Image.open(image_path).size[1]), int(Image.open(image_path).size[0]))\n",
    "        \n",
    "        center = (int(self.__annotation['annolist'][0, 0][0, img_idx]['annorect'][0, r_idx]['objpos'][0, 0]['y'][0, 0]),\n",
    "                    int(self.__annotation['annolist'][0, 0][0, img_idx]['annorect'][0, r_idx]['objpos'][0, 0]['x'][0, 0]))\n",
    "        length = int(self.__annotation['annolist'][0, 0][0, img_idx]['annorect'][0, r_idx]['scale'][0, 0] * 200 / 2)\n",
    "        \n",
    "        pad_y = [int(center[0]) - length, int(center[0]) + length]\n",
    "        pad_x = [int(center[1]) - length, int(center[1]) + length]\n",
    "        \n",
    "        if pad_y[0] >= 0:\n",
    "            pad_y[0] = 0\n",
    "        else:\n",
    "            pad_y[0] = -pad_y[0]\n",
    "            \n",
    "        if pad_y[1] <= image_res[0]:\n",
    "            pad_y[1] = 0\n",
    "        else:\n",
    "            pad_y[1] = pad_y[1] - image_res[0]\n",
    "            \n",
    "        if pad_x[0] >= 0:\n",
    "            pad_x[0] = 0\n",
    "        else:\n",
    "            pad_x[0] = -pad_x[0]\n",
    "            \n",
    "        if pad_x[1] <= image_res[1]:\n",
    "            pad_x[1] = 0\n",
    "        else:\n",
    "            pad_x[1] = pad_x[1] - image_res[1]\n",
    "        \n",
    "        return scipy.misc.imresize(\n",
    "            np.pad(\n",
    "                imageio.imread(\n",
    "                    os.path.join(\n",
    "                        self.__extract_path['image'],\n",
    "                        image_name\n",
    "                    )\n",
    "                ),\n",
    "                ((pad_y[0], pad_y[1]), (pad_x[0], pad_x[1]), (0, 0)),\n",
    "                'constant', constant_values = (0, 0)\n",
    "            )[center[0] + pad_y[0] - length:center[0] + pad_y[0] + length,\n",
    "              center[1] + pad_x[0] - length:center[1] + pad_x[0] + length,\n",
    "              :],\n",
    "            (256, 256)\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def __getHeat(self, index):\n",
    "        heatmaps = np.ndarray(shape = (64, 64, len(Joint)))\n",
    "        img_idx, r_idx = index\n",
    "        \n",
    "        image_name = self.__annotation['annolist'][0, 0][0, img_idx]['image'][0, 0]['name'][0]\n",
    "        image_path = os.path.join(self.__extract_path['image'], image_name)\n",
    "        image_res = (int(Image.open(image_path).size[1]), int(Image.open(image_path).size[0]))\n",
    "        \n",
    "        center = (int(self.__annotation['annolist'][0, 0][0, img_idx]['annorect'][0, r_idx]['objpos'][0, 0]['y'][0, 0]),\n",
    "                    int(self.__annotation['annolist'][0, 0][0, img_idx]['annorect'][0, r_idx]['objpos'][0, 0]['x'][0, 0]))\n",
    "        length = int(self.__annotation['annolist'][0, 0][0, img_idx]['annorect'][0, r_idx]['scale'][0, 0] * 200 / 2)\n",
    "        \n",
    "        pad_y = [int(center[0]) - length, int(center[0]) + length]\n",
    "        pad_x = [int(center[1]) - length, int(center[1]) + length]\n",
    "        \n",
    "        if pad_y[0] >= 0:\n",
    "            pad_y[0] = 0\n",
    "        else:\n",
    "            pad_y[0] = -pad_y[0]\n",
    "            \n",
    "        if pad_x[0] >= 0:\n",
    "            pad_x[0] = 0\n",
    "        else:\n",
    "            pad_x[0] = -pad_x[0]\n",
    "        \n",
    "        keypoints_ref = self.__annotation['annolist'][0, 0][0, img_idx]['annorect'][0, r_idx]['annopoints'][0, 0]['point']\n",
    "        \n",
    "        for joint in Joint:\n",
    "            heatmaps[:, :, joint.value - 1] = 0\n",
    "            if joint in MPII.joint2index:\n",
    "                n_joint = keypoints_ref.shape[1]\n",
    "                # print('n_joint:', n_joint)\n",
    "                for joint_idx in range(n_joint):\n",
    "                    try:\n",
    "                        visible = not (not keypoints_ref[0, joint_idx]['is_visible']\n",
    "                            or keypoints_ref[0, joint_idx]['is_visible'] == '0'\n",
    "                            or keypoints_ref[0, joint_idx]['is_visible'] == 0)\n",
    "                    except:\n",
    "                        visible = True\n",
    "                    tag = keypoints_ref[0, joint_idx]['id'][0, 0]\n",
    "                    \n",
    "                    if MPII.joint2index[joint] != tag or visible == False:\n",
    "                        continue\n",
    "                        \n",
    "                    scale = 64 / (length*2)\n",
    "                    heatmaps[:, :, joint.value - 1] = 255 * MPII.__makeGaussian(64, 1,\n",
    "                          [(int(keypoints_ref[0, joint_idx]['x'][0, 0])\n",
    "                            + int(length)\n",
    "                            - int(center[1])) * scale,\n",
    "                          (int(keypoints_ref[0, joint_idx]['y'][0, 0]) \n",
    "                          + int(length) \n",
    "                          - int(center[0])) * scale])\n",
    "                    \n",
    "        return heatmaps\n",
    "    \n",
    "    def __makeGaussian(size, sigma = 1, center = None):\n",
    "\n",
    "        x = np.arange(0, size, 1, np.float32)\n",
    "        y = x[:,np.newaxis]\n",
    "\n",
    "        if center is None:\n",
    "            x0 = y0 = size // 2\n",
    "        else:\n",
    "            x0 = center[0]\n",
    "            y0 = center[1]\n",
    "\n",
    "        return 1/(sigma*np.sqrt(2*np.pi))*np.exp(-((x-x0)**2 + (y-y0)**2) / 2*sigma**2)\n",
    "    \n",
    "    def __getExtra(self, index):\n",
    "        img_idx, r_idx = index\n",
    "        \n",
    "        image_name = self.__annotation['annolist'][0, 0][0, img_idx]['image'][0, 0]['name'][0]\n",
    "        image_path = os.path.join(self.__extract_path['image'], image_name)\n",
    "        image_res = (int(Image.open(image_path).size[1]), int(Image.open(image_path).size[0]))\n",
    "        length = int(self.__annotation['annolist'][0, 0][0, img_idx]['annorect'][0, r_idx]['scale'][0, 0] * 200)\n",
    "        \n",
    "        human_ref = self.__annotation['annolist'][0, 0][0, img_idx]['annorect'][0, r_idx]\n",
    "        \n",
    "        return np.linalg.norm(\n",
    "            np.array([int(human_ref['y1'][0, 0]), int(human_ref['x1'][0, 0])])\n",
    "            - np.array([int(human_ref['y2'][0, 0]), int(human_ref['x2'][0, 0])])) * 64 / length\n",
    "        \n",
    "    \n",
    "    def __getMasking():\n",
    "        return [(lambda joint: joint in MPII.joint2index)(joint) for joint in Joint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check if the archive file is extracted...success!\n",
      "\timage path: /media/nulledge/UBUNTU 16_0/MPII/images\n",
      "\tannotation path: /media/nulledge/UBUNTU 16_0/MPII/mpii_human_pose_v1_u12_2\n",
      "Load the annotation file...success!\n",
      "Check if the index file is exists...success!\n",
      "\ttrain path: /media/nulledge/UBUNTU 16_0/MPII/train.txt\n",
      "\teval path: /media/nulledge/UBUNTU 16_0/MPII/eval.txt\n"
     ]
    }
   ],
   "source": [
    "dataset = DataCenter(root = FLAGS.path_to_data).request(data = FLAGS.name_of_data, task = FLAGS.task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nulledge/Workspace/virtual_env/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:201: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "/home/nulledge/Workspace/virtual_env/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:240: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "/home/nulledge/Workspace/virtual_env/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:239: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n"
     ]
    }
   ],
   "source": [
    "batch = dataset.getBatch(batch_size = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nulledge/Workspace/virtual_env/tensorflow/lib/python3.5/site-packages/imageio/core/util.py:104: UserWarning: Conversion from float32 to uint8, range [0.0, 255.0]\n",
      "  'range [{2}, {3}]'.format(dtype_str, out_type.__name__, mi, ma))\n",
      "/home/nulledge/Workspace/virtual_env/tensorflow/lib/python3.5/site-packages/imageio/core/util.py:104: UserWarning: Conversion from float32 to uint8, range [0.0, 99.91498565673828]\n",
      "  'range [{2}, {3}]'.format(dtype_str, out_type.__name__, mi, ma))\n",
      "/home/nulledge/Workspace/virtual_env/tensorflow/lib/python3.5/site-packages/imageio/core/util.py:104: UserWarning: Conversion from float32 to uint8, range [0.0, 99.39185333251953]\n",
      "  'range [{2}, {3}]'.format(dtype_str, out_type.__name__, mi, ma))\n",
      "/home/nulledge/Workspace/virtual_env/tensorflow/lib/python3.5/site-packages/imageio/core/util.py:104: UserWarning: Conversion from float32 to uint8, range [0.0, 99.41596221923828]\n",
      "  'range [{2}, {3}]'.format(dtype_str, out_type.__name__, mi, ma))\n",
      "/home/nulledge/Workspace/virtual_env/tensorflow/lib/python3.5/site-packages/imageio/core/util.py:104: UserWarning: Conversion from float32 to uint8, range [0.0, 101.61526489257812]\n",
      "  'range [{2}, {3}]'.format(dtype_str, out_type.__name__, mi, ma))\n",
      "/home/nulledge/Workspace/virtual_env/tensorflow/lib/python3.5/site-packages/imageio/core/util.py:104: UserWarning: Conversion from float32 to uint8, range [0.0, 251.0]\n",
      "  'range [{2}, {3}]'.format(dtype_str, out_type.__name__, mi, ma))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.353087]\n",
      "[True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "[13.339947]\n",
      "[True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "[13.340944]\n",
      "[True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "[13.348116]\n",
      "[True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nulledge/Workspace/virtual_env/tensorflow/lib/python3.5/site-packages/imageio/core/util.py:104: UserWarning: Conversion from float32 to uint8, range [0.0, 98.6314926147461]\n",
      "  'range [{2}, {3}]'.format(dtype_str, out_type.__name__, mi, ma))\n",
      "/home/nulledge/Workspace/virtual_env/tensorflow/lib/python3.5/site-packages/imageio/core/util.py:104: UserWarning: Conversion from float32 to uint8, range [0.0, 247.0]\n",
      "  'range [{2}, {3}]'.format(dtype_str, out_type.__name__, mi, ma))\n",
      "/home/nulledge/Workspace/virtual_env/tensorflow/lib/python3.5/site-packages/imageio/core/util.py:104: UserWarning: Conversion from float32 to uint8, range [0.0, 98.62423706054688]\n",
      "  'range [{2}, {3}]'.format(dtype_str, out_type.__name__, mi, ma))\n",
      "/home/nulledge/Workspace/virtual_env/tensorflow/lib/python3.5/site-packages/imageio/core/util.py:104: UserWarning: Conversion from float32 to uint8, range [11.0, 255.0]\n",
      "  'range [{2}, {3}]'.format(dtype_str, out_type.__name__, mi, ma))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.333333]\n",
      "[True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "[13.334069]\n",
      "[True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "[13.418282]\n",
      "[True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "[13.34121]\n",
      "[True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nulledge/Workspace/virtual_env/tensorflow/lib/python3.5/site-packages/imageio/core/util.py:104: UserWarning: Conversion from float32 to uint8, range [0.0, 101.491943359375]\n",
      "  'range [{2}, {3}]'.format(dtype_str, out_type.__name__, mi, ma))\n",
      "/home/nulledge/Workspace/virtual_env/tensorflow/lib/python3.5/site-packages/imageio/core/util.py:104: UserWarning: Conversion from float32 to uint8, range [0.0, 101.58526611328125]\n",
      "  'range [{2}, {3}]'.format(dtype_str, out_type.__name__, mi, ma))\n"
     ]
    }
   ],
   "source": [
    "image, heatmaps, extra, mask = batch\n",
    "\n",
    "for idx in range(8):\n",
    "\n",
    "    imageio.imwrite('img/image' + str(idx) + '.png', image[idx])\n",
    "\n",
    "    for x in range(64):\n",
    "        for y in range(64):\n",
    "            heatmaps[idx, y, x, 0] = max(heatmaps[idx, y, x, :])\n",
    "    imageio.imwrite('img/heat' + str(idx) + '.png', heatmaps[idx, :, :, 0])\n",
    "    print(extra[idx])\n",
    "    print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('input'):\n",
    "    images = tf.placeholder(\n",
    "        name = 'image',\n",
    "        dtype = tf.float32,\n",
    "        shape = [None, metadata.image.height, metadata.image.width, metadata.image.channel])\n",
    "    heatmaps_groundtruth = tf.placeholder(\n",
    "        name = 'heatmap_groundtruth',\n",
    "        dtype = tf.float32,\n",
    "        shape = [None, metadata.heatmap.height, metadata.heatmap.width, metadata.joint])\n",
    "    train = tf.placeholder(\n",
    "        name = 'train',\n",
    "        dtype = tf.bool,\n",
    "        shape = ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input size 256 * 256 * 3\n",
    "\n",
    "with tf.variable_scope('compress'):\n",
    "    with tf.variable_scope('conv_bn_relu'):\n",
    "        net = layer.conv(input = images, ksize = 7, kchannel = 64, kstride = 2) # 128 * 128 * 64\n",
    "        net = layer.bn(input = net, train = train)\n",
    "        net = layer.relu(input = net)\n",
    "\n",
    "    net = module.bottleneck(input = net, kchannel = 128, train = train, name = 'A') # 128 * 128 * 128\n",
    "    net = layer.pool(input = net) # 64 * 64 * 128\n",
    "    net = module.bottleneck(input = net, kchannel = 128, train = train, name = 'B') # 64 * 64 * 128\n",
    "    net = module.bottleneck(input = net, kchannel = 256, train = train, name = 'C') # 64 * 64 * 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tf_Spectrum:\n",
    "    Color = tf.constant([\n",
    "        [128, 0, 0],\n",
    "        [255, 0, 0],\n",
    "        [0, 255, 0],\n",
    "        [0, 255, 255],\n",
    "        [0, 0, 255]\n",
    "    ], dtype = tf.float64)\n",
    "\n",
    "def tf_gray2color(gray, spectrum = tf_Spectrum.Color):\n",
    "    indices = tf.floor_div(gray, 64)\n",
    "    \n",
    "    t = (gray - indices * 64) / (64)\n",
    "    t = tf.stack([t]*3, 2)\n",
    "    indices = tf.cast(indices, dtype = tf.int32)\n",
    "    \n",
    "    return (1-t)*tf.gather(spectrum, indices) + t*tf.gather(spectrum, indices+1)\n",
    "\n",
    "def tf_merge(image, heatmaps):\n",
    "    board = tf.zeros(\n",
    "        shape = (\n",
    "            metadata.heatmap.height,\n",
    "            metadata.heatmap.width,\n",
    "            metadata.image.channel\n",
    "        ), dtype = tf.float64)\n",
    "    for joint in range(metadata.joint):\n",
    "        board = tf.maximum(\n",
    "            board, \n",
    "            tf_gray2color(tf.cast(heatmaps[:, :, joint], dtype=tf.float64)))\n",
    "    board = tf.image.resize_images(\n",
    "        board, \n",
    "        [metadata.image.height, metadata.image.width])\n",
    "    image = tf.cast(image, dtype = tf.float32)\n",
    "    return tf.cast(tf.add(tf.multiply(board, 0.6), tf.multiply(image, 0.4)), dtype = tf.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_stage = 8\n",
    "heatmaps = []\n",
    "\n",
    "for stage in range(1, last_stage+1):\n",
    "    with tf.variable_scope('hourglass_' + str(stage)):\n",
    "        prev = tf.identity(net)\n",
    "        net = module.hourglass(input = net, train = train) # 64 * 64 * 256\n",
    "\n",
    "        with tf.variable_scope('inter_hourglasses'):\n",
    "            net = module.bottleneck(input = net, train = train) # 64 * 64 * 256\n",
    "            net = layer.conv(input = net, ksize = 1, kchannel = 256) # 64 * 64 * 256\n",
    "            net = layer.bn(input = net, train = train)\n",
    "            net = layer.relu(input = net)\n",
    "\n",
    "        with tf.variable_scope('heatmap'):\n",
    "            heatmap = layer.conv(input = net, ksize = 1, kchannel = metadata.joint) # 64 * 64 * joint\n",
    "            heatmaps.append(heatmap)\n",
    "\n",
    "        if stage != last_stage:\n",
    "            net = layer.conv(input = net, ksize = 1, kchannel = 256, name = 'inter')\\\n",
    "                + layer.conv(input = heatmap, ksize = 1, kchannel = 256, name = 'heatmap')\\\n",
    "                + prev # 64 * 64 * 256\n",
    "\n",
    "merged = tf_merge(images[0], heatmaps[-1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAGS.train :\n",
    "    with tf.variable_scope('loss'):\n",
    "        loss = tf.losses.mean_squared_error(heatmaps_groundtruth, heatmaps[0])\n",
    "        for stage in range(1, last_stage):\n",
    "            loss = loss + tf.losses.mean_squared_error(heatmaps_groundtruth, heatmaps[stage])\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            optimizer = tf.train.AdamOptimizer(name = 'optimizer', learning_rate = 0.00025).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "reader = Reader(train = os.path.expanduser('~/Temp/train19122.dat'),\n",
    "                test = os.path.expanduser('~/Temp/test2125.dat'))\n",
    "if FLAGS.train:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, FLAGS.ckpt)\n",
    "else:\n",
    "    saver.restore(sess, FLAGS.ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAGS.train == True:\n",
    "    for epoch in range(100):\n",
    "        train_iter = tqdm_notebook(total = 19122, desc = 'epoch: ' + str(epoch) + '/100')\n",
    "        for i in range(3187):\n",
    "            train_images, train_heatmaps = reader.batch(size = FLAGS.batch, is_train = True)\n",
    "            _, result = sess.run([optimizer, loss],\n",
    "                feed_dict = {\n",
    "                    images: train_images,\n",
    "                    heatmaps_groundtruth: train_heatmaps,\n",
    "                    train: True})\n",
    "            train_iter.set_postfix(loss = result)\n",
    "            train_iter.update(FLAGS.batch)\n",
    "        train_iter.close();\n",
    "        temp = saver.save(sess, FLAGS.ckpt)\n",
    "    '''\n",
    "    for epoch in tqdm_notebook(tqdm(range(10), desc = 'epoch')):\n",
    "        inner_iter = tqdm_notebook(tqdm(range(563), desc = 'iter'), leave = False)\n",
    "        for iterator in inner_iter:\n",
    "            train_images, train_heatmaps = reader.batch(size = flag.batch_size, is_train = True)\n",
    "            _, result = sess.run([optimizer, loss],\n",
    "                feed_dict = {\n",
    "                    images: train_images,\n",
    "                    heatmaps_groundtruth: train_heatmaps,\n",
    "                    train: True})\n",
    "            wrap = lambda label, value: label + '(' + str(value) + ')'\n",
    "            inner_iter.set_postfix(loss = result)\n",
    "        temp = saver.save(sess, os.path.join(ckpt_path, 'hourglass_' + str(start_epoch + epoch + 1) + '.ckpt'))\n",
    "        print(\"Model saved in file: %s\" % temp)\n",
    "        '''\n",
    "else:\n",
    "    heatmap_idx = 0\n",
    "    total_result = []\n",
    "    test_iter = tqdm_notebook(total = 2125, desc = 'test')\n",
    "    for i in range(2125 // 5):\n",
    "        test_images, test_heatmaps = reader.batch(size = 5, is_train = False)\n",
    "        result, output_heatmap = sess.run([loss, merged],\n",
    "            feed_dict = {\n",
    "                images: test_images,\n",
    "                heatmaps_groundtruth: test_heatmaps,\n",
    "                train: False})\n",
    "        test_iter.update(5)\n",
    "        total_result.append(result)\n",
    "        cv2.imwrite('merged' + str(heatmap_idx) + '.jpg', output_heatmap)\n",
    "        heatmap_idx += 1\n",
    "    test_iter.close();\n",
    "    print(sum(total_result)/(2125//5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_idx = 0\n",
    "total_result = []\n",
    "test_iter = tqdm_notebook(total = 2125, desc = 'test')\n",
    "for i in range(2125 // 5):\n",
    "    test_images, test_heatmaps = reader.batch(size = 5, is_train = False)\n",
    "    result, output_heatmap = sess.run([loss, merged],\n",
    "        feed_dict = {\n",
    "            images: test_images,\n",
    "            heatmaps_groundtruth: test_heatmaps,\n",
    "            train: False})\n",
    "    test_iter.update(5)\n",
    "    total_result.append(result)\n",
    "    cv2.imwrite('merged' + str(heatmap_idx) + '.jpg', output_heatmap)\n",
    "    heatmap_idx += 1\n",
    "test_iter.close();\n",
    "print(sum(total_result)/(2125//5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
